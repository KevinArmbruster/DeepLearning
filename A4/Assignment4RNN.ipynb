{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:39:56.258677Z",
     "start_time": "2023-05-01T14:39:56.228524Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:39:56.333994Z",
     "start_time": "2023-05-01T14:39:56.266969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookPath = \"goblet_book.txt\"\n",
    "\n",
    "with open(bookPath) as f:\n",
    "    bookContent = f.read() #string\n",
    "\n",
    "alphabet = sorted(set(bookContent))\n",
    "K = len(alphabet)\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:39:56.390488Z",
     "start_time": "2023-05-01T14:39:56.340500Z"
    }
   },
   "outputs": [],
   "source": [
    "index_to_char = {i: char for i, char in enumerate(alphabet)}\n",
    "char_to_index = {char: i for i, char in enumerate(alphabet)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T14:39:56.398493Z",
     "start_time": "2023-05-01T14:39:56.357202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 2)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Hi\n"
     ]
    }
   ],
   "source": [
    "def string2vec(string):\n",
    "    vec = np.array([char_to_index[c] for c in string])\n",
    "    return vec\n",
    "\n",
    "def oneHotEncode(idx):\n",
    "    N = len(idx) if type(idx) is np.ndarray else 1\n",
    "    one_hot_matrix = np.zeros((N, K))\n",
    "    one_hot_matrix[np.arange(N), idx] = 1\n",
    "    return one_hot_matrix.T\n",
    "\n",
    "def string2oneHot(string):\n",
    "    vec = string2vec(string)\n",
    "    return oneHotEncode(vec)\n",
    "\n",
    "def oneHot2String(oneHot):\n",
    "    vec = np.argmax(oneHot, axis=0)\n",
    "    string = ''.join([index_to_char[idx] for idx in vec])\n",
    "    return string\n",
    "\n",
    "oneHot = string2oneHot(\"Hi\")\n",
    "print(oneHot.shape)\n",
    "print(oneHot.T)\n",
    "\n",
    "string = oneHot2String(oneHot)\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:08:44.185140Z",
     "start_time": "2023-05-01T15:08:41.978546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 25)\n",
      "(83, 25)\n",
      "e Sdl 0d GjCMQ\"YeobB/â‚¬Plv\n",
      "Computing numerical gradient for\n",
      "Field name: b\n",
      "Computing numerical gradient for\n",
      "Field name: c\n",
      "Computing numerical gradient for\n",
      "Field name: U\n",
      "Computing numerical gradient for\n",
      "Field name: W\n",
      "Computing numerical gradient for\n",
      "Field name: V\n",
      "Asserting b\n",
      "Asserting c\n",
      "Asserting U\n",
      "Asserting W\n",
      "Asserting V\n",
      "Loss = 105.40734768420859\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global num_grads\n",
    "#num_grads = Grads(1,1)\n",
    "\n",
    "class Weights:\n",
    "    def __init__(self, chars, hiddenDim, sig):\n",
    "        self.b = np.zeros((hiddenDim, 1))\n",
    "        self.c = np.zeros((chars, 1))\n",
    "        self.U = np.random.random((hiddenDim, chars)) * sig\n",
    "        self.W = np.random.random((hiddenDim, hiddenDim)) * sig\n",
    "        self.V = np.random.random((chars, hiddenDim)) * sig\n",
    "\n",
    "class Grads:\n",
    "    def __init__(self, chars, hiddenDim):\n",
    "        self.b = np.zeros((hiddenDim, 1))\n",
    "        self.c = np.zeros((chars, 1))\n",
    "        self.U = np.zeros((hiddenDim, chars))\n",
    "        self.W = np.zeros((hiddenDim, hiddenDim))\n",
    "        self.V = np.zeros((chars, hiddenDim))\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, chars, hiddenDim, seqLength, eta, sig = 0.01):\n",
    "        self.chars = chars\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.seqLength = seqLength\n",
    "        self.eta = eta\n",
    "        self.sig = sig\n",
    "\n",
    "        self.weights = Weights(chars, hiddenDim, sig)\n",
    "        self.grads = Grads(chars, hiddenDim)\n",
    "\n",
    "        self.h0 = np.zeros((hiddenDim, 1))\n",
    "        self.dummy = \".\"\n",
    "\n",
    "        self.cache = defaultdict(list)\n",
    "\n",
    "    def relu(self, X, d=False):\n",
    "        if not d:\n",
    "            return np.maximum(0, X)\n",
    "            # np.where(X > 0, X, 0)\n",
    "        else:\n",
    "            return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def tanh(self, X, d=False):\n",
    "        if not d:\n",
    "            return (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "        else:\n",
    "            return 1 - self.tanh(X)**2\n",
    "\n",
    "\n",
    "    def softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Standard definition of the softmax function \"\"\"\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def SingleForwardPass(self, X, h_prev):\n",
    "        assert X.shape[0] == self.chars\n",
    "\n",
    "        a = self.weights.W @ h_prev    # 100x100 @ 100x1 = 100x1\n",
    "        a += self.weights.U @ X        # + 100x83 @ 83x1 = 100x1\n",
    "        a += self.weights.b            # + 100x1\n",
    "        h = self.tanh(a)\n",
    "\n",
    "        o = self.weights.V @ h + self.weights.c\n",
    "        p = self.softmax(o)\n",
    "\n",
    "        assert h.shape[0] == self.hiddenDim\n",
    "        assert p.shape[0] == self.chars\n",
    "\n",
    "        return p, h\n",
    "\n",
    "    def Sample(self, P):\n",
    "        cp = np.cumsum(P)\n",
    "        a = np.random.rand()\n",
    "        ixs = np.where(cp - a > 0)\n",
    "        ii = ixs[0][0]\n",
    "        return ii\n",
    "\n",
    "    def ComputeLoss(self, X, Y_true):\n",
    "        Y_pred = self.ForwardPass(X)\n",
    "\n",
    "        # loss over all time steps\n",
    "        _loss = - Y_true * np.log(Y_pred)\n",
    "        loss = np.sum(_loss, axis=0).sum()  # mean???\n",
    "\n",
    "        return np.squeeze(loss)\n",
    "\n",
    "    def ComputeLossForNumGrad(self, X, Y_true, weigths_try, h0):\n",
    "        safe = copy.deepcopy(self)\n",
    "\n",
    "        self.weights = weigths_try\n",
    "        self.h0 = h0\n",
    "\n",
    "        res = self.ComputeLoss(X, Y_true)\n",
    "\n",
    "        # reset everything^^\n",
    "        self.cache = safe.cache\n",
    "        self.weights = safe.weights\n",
    "        self.grads = safe.grads\n",
    "        self.h0 = safe.h0\n",
    "\n",
    "        return res\n",
    "\n",
    "    def SynthesizeText(self, seqLength):\n",
    "        prediction = []\n",
    "        x = string2oneHot(self.dummy)\n",
    "        h = self.h0\n",
    "\n",
    "        for i in range(seqLength):\n",
    "            p, _ = self.SingleForwardPass(x, h)\n",
    "            idx = self.Sample(p)\n",
    "            prediction.append(index_to_char[idx])\n",
    "            x = oneHotEncode(idx)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def ForwardPass(self, X):\n",
    "        assert X.shape == (self.chars, self.seqLength)\n",
    "\n",
    "        self.cache = defaultdict(list)\n",
    "\n",
    "        predictions = []\n",
    "        hiddens = []\n",
    "        h = self.h0\n",
    "\n",
    "        for t in range(self.seqLength):\n",
    "            x = X[:, t].reshape(-1, 1)\n",
    "\n",
    "            self.cache[\"x\"].append(x)\n",
    "            self.cache[\"h_prev\"].append(h)\n",
    "\n",
    "            p, h = self.SingleForwardPass(x, h)\n",
    "            hiddens.append(h)\n",
    "            predictions.append(p)\n",
    "\n",
    "            self.cache[\"h\"].append(h)\n",
    "            self.cache[\"p\"].append(p)\n",
    "\n",
    "        predictions = np.asarray(predictions).squeeze().T\n",
    "        hiddens = np.asarray(hiddens).squeeze().T\n",
    "\n",
    "        assert predictions.shape == (self.chars, self.seqLength)\n",
    "        assert hiddens.shape == (self.hiddenDim, self.seqLength)\n",
    "\n",
    "        return predictions #, hiddens\n",
    "\n",
    "    def BackwardPassSingle(self, Y_pred, Y_true, dh_next, h_next, h_prev, xt):\n",
    "        dy = Y_pred - Y_true\n",
    "\n",
    "        # gradients wrt y\n",
    "        self.grads.V += dy @ h_next.T\n",
    "        self.grads.c += dy\n",
    "        dh = np.dot(self.weights.V.T, dy) + dh_next\n",
    "\n",
    "        # gradients wrt hidden\n",
    "        dtanh = self.tanh(dh, d=True)\n",
    "        self.grads.b += dtanh\n",
    "        self.grads.U += dtanh @ xt.T\n",
    "        self.grads.W += dtanh @ h_prev.T\n",
    "\n",
    "        dh_next = self.weights.W.T @ dtanh\n",
    "        return dh_next\n",
    "\n",
    "\n",
    "    def BackwardPass(self, X, Y_pred, Y_true):\n",
    "        assert X.shape == Y_pred.shape == Y_true.shape == (self.chars, self.seqLength)\n",
    "\n",
    "        self.grads = Grads(self.chars, self.hiddenDim) # reset grads\n",
    "\n",
    "        dY = Y_pred - Y_true    # 83x25\n",
    "\n",
    "        for i in range(self.seqLength):\n",
    "            self.grads.V += dY[:, i, None] @ self.cache[\"h\"][i+1].T # gt and ht\n",
    "        self.grads.c = np.sum(dY, axis=1, keepdims=True)\n",
    "\n",
    "        assert self.grads.V.shape == self.weights.V.shape # 83x1 @ 1x100 = 83x100\n",
    "        assert self.grads.c.shape == self.weights.c.shape # 83x1\n",
    "\n",
    "        dH = [None] * self.seqLength\n",
    "        dA = [None] * self.seqLength\n",
    "        for i in reversed(range(self.seqLength-1)):\n",
    "            # dL / dh = sum of\n",
    "            ## dL / do\n",
    "            dH[i] = dY[:, i, None].T @ self.weights.V     # 1x83 @ 83x100 = 1x100\n",
    "            ## + dL / da_t+1\n",
    "            dH[i] += self.cache[\"a\"][i+1].T @ self.weights.W    # 1x100 @ 100x100 = 1x100\n",
    "            assert dH[i].shape == (1, self.hiddenDim)\n",
    "\n",
    "            # dL / da_t\n",
    "            dA[i] = dH[i] @ np.diag(1 - np.tanh(self.cache[\"a\"][i][:,0])**2)  # 1x100 @ 100x100 = 1x100\n",
    "            assert dA[i].shape == (1, self.hiddenDim)\n",
    "\n",
    "        for i in range(self.seqLength-1):\n",
    "            self.grads.W += dA[i].T @ self.cache[\"h\"][i].T # gt and ht-1    # 100x1 @ 1x100 = 100x100\n",
    "            self.grads.U += dA[i].T @ X[:, i, None].T                # 100x1 @ 1x83 = 100x83\n",
    "            self.grads.b += dA[i].T                                       # 100x1\n",
    "\n",
    "        assert self.grads.W.shape == self.weights.W.shape\n",
    "        assert self.grads.U.shape == self.weights.U.shape\n",
    "        assert self.grads.b.shape == self.weights.b.shape\n",
    "\n",
    "        # self.ClipGradients()\n",
    "        return self.grads\n",
    "\n",
    "    def ComputeGradsNum(self, X, Y, h=1e-4):\n",
    "        num_grads = Grads(self.chars, self.hiddenDim)\n",
    "\n",
    "        for f in self.weights.__dict__.keys():\n",
    "            print('Computing numerical gradient for')\n",
    "            print(f'Field name: {f}')\n",
    "            num_grads.__dict__[f] = self.ComputeGradNum(X, Y, f, h)\n",
    "        return num_grads\n",
    "\n",
    "    def ComputeGradNum(self, X, Y, f, h):\n",
    "        grad = np.zeros_like(self.weights.__dict__[f])\n",
    "        h0 = self.h0\n",
    "\n",
    "        for i in np.ndindex(self.weights.__dict__[f].shape):\n",
    "            weights_try = copy.deepcopy(self.weights)\n",
    "            weights_try.__dict__[f][i] -= h\n",
    "            l1 = self.ComputeLossForNumGrad(X, Y, weights_try, h0)\n",
    "            weights_try.__dict__[f][i] += 2 * h\n",
    "            l2 = self.ComputeLossForNumGrad(X, Y, weights_try, h0)\n",
    "            grad[i] = (l2 - l1) / (2 * h)\n",
    "        return grad\n",
    "\n",
    "    def ClipGradients(self, t = 5):\n",
    "        for f in self.grads.__dict__.keys():\n",
    "            self.grads.__dict__[f] = max(min(self.grads.__dict__[f], t), -t)\n",
    "\n",
    "    def SGDStep(self):\n",
    "        for f in self.weights.__dict__.keys():\n",
    "            self.weights.__dict__[f] -= self.eta * self.grads.__dict__[f]\n",
    "\n",
    "    def Fit(self, X, Y):\n",
    "        Y_pred = self.ForwardPass(X)\n",
    "        grads = self.BackwardPass(X, Y_pred, Y)\n",
    "        global num_grads\n",
    "        num_grads = self.ComputeGradsNum(X, Y)\n",
    "\n",
    "        for f in self.grads.__dict__.keys():\n",
    "            print(f\"Asserting {f}\")\n",
    "            # assert np.allclose(grads.__dict__[f], num_grads.__dict__[f], atol=1e-3)\n",
    "\n",
    "        self.SGDStep()\n",
    "\n",
    "        loss = self.ComputeLoss(X, Y)\n",
    "        print(f\"Loss = {loss}\")\n",
    "\n",
    "\n",
    "rnn = RNN(K, 5, 25, 1e-1)\n",
    "\n",
    "X_ = bookContent[0:rnn.seqLength]\n",
    "Y_ = bookContent[1:rnn.seqLength + 1]\n",
    "\n",
    "# print(X_)\n",
    "# print(Y_)\n",
    "\n",
    "X_ = string2oneHot(X_)\n",
    "Y_ = string2oneHot(Y_)\n",
    "print(X_.shape)\n",
    "print(Y_.shape)\n",
    "\n",
    "seq = rnn.SynthesizeText(rnn.seqLength)\n",
    "print(\"\".join(seq))\n",
    "\n",
    "rnn.Fit(X_, Y_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:09:02.685930Z",
     "start_time": "2023-05-01T15:09:02.645940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.39757930e-03,  1.37007603e-03,  1.34184319e-03,\n",
       "         1.03167658e-03,  1.44539122e-03],\n",
       "       [ 1.39773022e-03,  1.37023363e-03,  1.34195716e-03,\n",
       "         1.03178749e-03,  1.44552338e-03],\n",
       "       [-2.07766846e-02, -1.09043265e-02, -1.65785935e-02,\n",
       "        -8.76061257e-03, -2.44227152e-02],\n",
       "       [ 1.39765490e-03,  1.37017352e-03,  1.34191325e-03,\n",
       "         1.03173662e-03,  1.44546185e-03],\n",
       "       [ 1.39768957e-03,  1.37018176e-03,  1.34193570e-03,\n",
       "         1.03175175e-03,  1.44550413e-03],\n",
       "       [ 1.39767764e-03,  1.37018255e-03,  1.34191311e-03,\n",
       "         1.03174912e-03,  1.44547215e-03],\n",
       "       [ 1.39760779e-03,  1.37009543e-03,  1.34185427e-03,\n",
       "         1.03169057e-03,  1.44541957e-03],\n",
       "       [ 1.39773682e-03,  1.37022305e-03,  1.34197869e-03,\n",
       "         1.03178721e-03,  1.44554555e-03],\n",
       "       [ 1.39775914e-03,  1.37027243e-03,  1.34200242e-03,\n",
       "         1.03180966e-03,  1.44557035e-03],\n",
       "       [ 1.39763493e-03,  1.37013821e-03,  1.34189705e-03,\n",
       "         1.03172233e-03,  1.44544501e-03],\n",
       "       [ 1.39767984e-03,  1.37017864e-03,  1.34193236e-03,\n",
       "         1.03175395e-03,  1.44548075e-03],\n",
       "       [ 1.39769469e-03,  1.37020287e-03,  1.34192298e-03,\n",
       "         1.03176021e-03,  1.44548849e-03],\n",
       "       [ 1.39766485e-03,  1.37016947e-03,  1.34191026e-03,\n",
       "         1.03174145e-03,  1.44546227e-03],\n",
       "       [ 1.39768751e-03,  1.37018880e-03,  1.34192788e-03,\n",
       "         1.03175111e-03,  1.44549780e-03],\n",
       "       [ 1.39768680e-03,  1.37018596e-03,  1.34193584e-03,\n",
       "         1.03175317e-03,  1.44549553e-03],\n",
       "       [ 1.39768765e-03,  1.37018624e-03,  1.34192888e-03,\n",
       "         1.03175033e-03,  1.44549574e-03],\n",
       "       [ 1.39767977e-03,  1.37019640e-03,  1.34193250e-03,\n",
       "         1.03175623e-03,  1.44548345e-03],\n",
       "       [ 1.39767799e-03,  1.37019768e-03,  1.34192931e-03,\n",
       "         1.03175140e-03,  1.44548736e-03],\n",
       "       [ 1.39766364e-03,  1.37017857e-03,  1.34192724e-03,\n",
       "         1.03174479e-03,  1.44547847e-03],\n",
       "       [ 1.39761887e-03,  1.37012293e-03,  1.34188021e-03,\n",
       "         1.03170969e-03,  1.44542291e-03],\n",
       "       [ 1.39761880e-03,  1.37012066e-03,  1.34187871e-03,\n",
       "         1.03171274e-03,  1.44541985e-03],\n",
       "       [ 1.39765731e-03,  1.37017366e-03,  1.34190863e-03,\n",
       "         1.03174408e-03,  1.44545098e-03],\n",
       "       [ 1.39764772e-03,  1.37015981e-03,  1.34189627e-03,\n",
       "         1.03173058e-03,  1.44545339e-03],\n",
       "       [-1.30041968e-02, -8.55918763e-03, -5.00516272e-03,\n",
       "        -1.08198610e-02, -3.07394011e-03],\n",
       "       [-5.93752105e-03, -7.30436525e-03, -6.38171457e-03,\n",
       "        -3.92762118e-03, -5.68706405e-03],\n",
       "       [ 1.39763650e-03,  1.37014119e-03,  1.34188568e-03,\n",
       "         1.03171963e-03,  1.44544479e-03],\n",
       "       [-1.82106014e-03, -7.93616650e-03,  7.75320643e-04,\n",
       "        -3.99346050e-04, -3.67804169e-03],\n",
       "       [-1.47598234e-02, -1.21311906e-02, -5.61159794e-03,\n",
       "        -1.08942991e-02, -8.95626400e-03],\n",
       "       [ 1.39768389e-03,  1.37019079e-03,  1.34193193e-03,\n",
       "         1.03175623e-03,  1.44548331e-03],\n",
       "       [-4.26723844e-03,  3.17303019e-04, -3.73446511e-03,\n",
       "        -3.16334656e-03, -1.03966983e-03],\n",
       "       [ 6.02281602e-05, -2.07202483e-03, -3.83678241e-03,\n",
       "        -2.55412417e-03, -1.60010167e-03],\n",
       "       [ 1.39767160e-03,  1.37016514e-03,  1.34191900e-03,\n",
       "         1.03174060e-03,  1.44547556e-03],\n",
       "       [ 1.39763522e-03,  1.37012805e-03,  1.34188141e-03,\n",
       "         1.03171409e-03,  1.44543982e-03],\n",
       "       [ 1.39765440e-03,  1.37017885e-03,  1.34190294e-03,\n",
       "         1.03174003e-03,  1.44544941e-03],\n",
       "       [-4.35109087e-04, -7.05186139e-03, -4.97538679e-03,\n",
       "        -2.51778857e-03,  9.54064205e-05],\n",
       "       [ 1.39763380e-03,  1.37014339e-03,  1.34187800e-03,\n",
       "         1.03171786e-03,  1.44543179e-03],\n",
       "       [ 2.09099724e-04, -5.30825510e-03, -3.53713325e-03,\n",
       "        -3.66846010e-03, -3.62621719e-03],\n",
       "       [-5.11636614e-03, -1.41939640e-02, -9.81976392e-03,\n",
       "        -1.63356972e-03, -9.22926255e-03],\n",
       "       [-4.19532448e-03,  3.64461101e-04, -3.67065368e-03,\n",
       "        -3.09543488e-03, -9.72946452e-04],\n",
       "       [ 1.39771771e-03,  1.37022298e-03,  1.34196071e-03,\n",
       "         1.03177292e-03,  1.44553120e-03],\n",
       "       [-1.17460203e-02, -9.15909318e-03, -1.10232376e-02,\n",
       "        -8.13106098e-03, -2.07227598e-02],\n",
       "       [ 1.39768261e-03,  1.37019022e-03,  1.34191936e-03,\n",
       "         1.03175225e-03,  1.44548196e-03],\n",
       "       [-1.30526158e-02, -1.19137188e-02, -1.67456121e-02,\n",
       "        -1.18534011e-02, -1.13348167e-02],\n",
       "       [ 1.39770179e-03,  1.37019313e-03,  1.34193996e-03,\n",
       "         1.03176269e-03,  1.44550860e-03],\n",
       "       [ 1.39763500e-03,  1.37015981e-03,  1.34190060e-03,\n",
       "         1.03172269e-03,  1.44545069e-03],\n",
       "       [ 1.39771515e-03,  1.37020592e-03,  1.34195631e-03,\n",
       "         1.03177172e-03,  1.44552381e-03],\n",
       "       [ 1.39761617e-03,  1.37012954e-03,  1.34188639e-03,\n",
       "         1.03170791e-03,  1.44543087e-03],\n",
       "       [-1.59645452e-03,  1.31049759e-03, -2.44749273e-03,\n",
       "         2.28684058e-04, -5.48930075e-03],\n",
       "       [ 1.39773206e-03,  1.37023548e-03,  1.34197379e-03,\n",
       "         1.03178579e-03,  1.44554349e-03],\n",
       "       [ 1.39769767e-03,  1.37020329e-03,  1.34194778e-03,\n",
       "         1.03176461e-03,  1.44551045e-03],\n",
       "       [ 1.39764694e-03,  1.37014958e-03,  1.34190167e-03,\n",
       "         1.03172574e-03,  1.44546028e-03],\n",
       "       [ 1.39766826e-03,  1.37017672e-03,  1.34191644e-03,\n",
       "         1.03174088e-03,  1.44547464e-03],\n",
       "       [ 1.39763266e-03,  1.37014268e-03,  1.34188220e-03,\n",
       "         1.03171544e-03,  1.44544217e-03],\n",
       "       [ 1.39764829e-03,  1.37014496e-03,  1.34191282e-03,\n",
       "         1.03172916e-03,  1.44546668e-03],\n",
       "       [ 1.39773874e-03,  1.37024749e-03,  1.34199084e-03,\n",
       "         1.03179708e-03,  1.44555209e-03],\n",
       "       [ 1.39766030e-03,  1.37015206e-03,  1.34190742e-03,\n",
       "         1.03173633e-03,  1.44546426e-03],\n",
       "       [ 1.39768161e-03,  1.37017317e-03,  1.34191552e-03,\n",
       "         1.03174827e-03,  1.44548061e-03],\n",
       "       [ 1.39771714e-03,  1.37023818e-03,  1.34196334e-03,\n",
       "         1.03178380e-03,  1.44552459e-03],\n",
       "       [ 1.39760999e-03,  1.37011355e-03,  1.34187403e-03,\n",
       "         1.03170542e-03,  1.44541481e-03],\n",
       "       [ 1.39768503e-03,  1.37019597e-03,  1.34192433e-03,\n",
       "         1.03175282e-03,  1.44549375e-03],\n",
       "       [ 1.39770790e-03,  1.37021672e-03,  1.34194373e-03,\n",
       "         1.03176781e-03,  1.44551194e-03],\n",
       "       [ 1.39763280e-03,  1.37014311e-03,  1.34189420e-03,\n",
       "         1.03171793e-03,  1.44544202e-03],\n",
       "       [ 1.39770130e-03,  1.37021644e-03,  1.34195261e-03,\n",
       "         1.03177129e-03,  1.44550590e-03],\n",
       "       [ 1.39760175e-03,  1.37010673e-03,  1.34185690e-03,\n",
       "         1.03169278e-03,  1.44540998e-03],\n",
       "       [ 1.39765703e-03,  1.37014744e-03,  1.34190770e-03,\n",
       "         1.03173434e-03,  1.44545780e-03],\n",
       "       [ 1.39767728e-03,  1.37019832e-03,  1.34193449e-03,\n",
       "         1.03175118e-03,  1.44549361e-03],\n",
       "       [ 1.39761489e-03,  1.37012663e-03,  1.34187623e-03,\n",
       "         1.03170741e-03,  1.44542064e-03],\n",
       "       [ 1.39767067e-03,  1.37017778e-03,  1.34192867e-03,\n",
       "         1.03174791e-03,  1.44547727e-03],\n",
       "       [ 1.39767721e-03,  1.37017487e-03,  1.34192113e-03,\n",
       "         1.03174344e-03,  1.44548537e-03],\n",
       "       [ 1.39764900e-03,  1.37015931e-03,  1.34191367e-03,\n",
       "         1.03173178e-03,  1.44546640e-03],\n",
       "       [ 1.39762442e-03,  1.37013245e-03,  1.34187701e-03,\n",
       "         1.03171296e-03,  1.44542661e-03],\n",
       "       [ 1.39767593e-03,  1.37019683e-03,  1.34192057e-03,\n",
       "         1.03175481e-03,  1.44547371e-03],\n",
       "       [ 1.39768751e-03,  1.37019235e-03,  1.34193215e-03,\n",
       "         1.03176077e-03,  1.44548231e-03],\n",
       "       [ 1.39768403e-03,  1.37017231e-03,  1.34193186e-03,\n",
       "         1.03175346e-03,  1.44548693e-03],\n",
       "       [ 1.39765291e-03,  1.37015121e-03,  1.34189456e-03,\n",
       "         1.03172972e-03,  1.44545531e-03],\n",
       "       [ 1.39766961e-03,  1.37016976e-03,  1.34191993e-03,\n",
       "         1.03174514e-03,  1.44547492e-03],\n",
       "       [ 1.39772915e-03,  1.37020898e-03,  1.34196370e-03,\n",
       "         1.03178060e-03,  1.44553496e-03],\n",
       "       [ 1.39767621e-03,  1.37017587e-03,  1.34191360e-03,\n",
       "         1.03174884e-03,  1.44547336e-03],\n",
       "       [ 1.39771863e-03,  1.37020876e-03,  1.34195396e-03,\n",
       "         1.03177172e-03,  1.44552679e-03],\n",
       "       [ 1.39764836e-03,  1.37015689e-03,  1.34190159e-03,\n",
       "         1.03173150e-03,  1.44545339e-03],\n",
       "       [ 1.39766058e-03,  1.37015846e-03,  1.34191957e-03,\n",
       "         1.03173832e-03,  1.44547357e-03],\n",
       "       [ 1.39757390e-03,  1.37007746e-03,  1.34183786e-03,\n",
       "         1.03167778e-03,  1.44537701e-03],\n",
       "       [ 1.39765774e-03,  1.37016322e-03,  1.34190245e-03,\n",
       "         1.03173520e-03,  1.44545403e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_grads.__dict__[\"V\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:09:57.115993Z",
     "start_time": "2023-05-01T15:09:57.084236Z"
    }
   },
   "outputs": [],
   "source": [
    "type(num_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:10:14.421233Z",
     "start_time": "2023-05-01T15:10:14.395365Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"aa\"+type(num_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:10:37.699879Z",
     "start_time": "2023-05-01T15:10:37.630159Z"
    }
   },
   "outputs": [],
   "source": [
    "def CreateData(bookContent, seqLength):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(len(bookContent) - seqLength):\n",
    "        X.append(bookContent[i    : i + seqLength])\n",
    "        Y.append(bookContent[i + 1: i + seqLength + 1])\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    X = string2oneHot(X) # would result in list of matrices? do i want that?\n",
    "\n",
    "    Y = np.asarray(Y)\n",
    "    Y = string2oneHot(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# X, Y = CreateData(bookContent, rnn.seqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T15:10:51.426411Z",
     "start_time": "2023-05-01T15:10:51.404746Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
